## Kubernetes

- We have access to the National Science Foundation Nautilus HyperCluster through Mizzou's High performance data intensive computing systems lab. This cluster is managed through Kubernetes, an open-source platform to automate deployment, scaling, and operating application containers.

- There is an abundance of useful information, tutorials, helpful hints, and examples at the [Mizzou HPC github](https://github.com/MU-HPDI).

- Additional help is available using [Matrix Chat](https://nationalresearchplatform.org/updates/matrix-chat-for-nautilus-users/).
  
## Installing Kubernetes 

Install kubernetes on Ubuntu

- `sudo snap install kubectl --classic`
- Verify installation with `kubectl`. This should show the command is recogized by system.
  
Set up config file for computing cluster

- [Nautilus](https://portal.nrp-nautilus.io) is the computing cluster for this project. Make an account and login. Then download the config file and place inside of `~/.kube/` on the client machine.

- Verify config file is setup correctly via `kubectl get pods`. It should not throw warnings or errors.

- Our namespace is called gpn-mizzou-muem.

## Using Kubernetes in this repo

### Kubernetes storage

- We use persistent volume claims (PVCs) for storage on the Nautilus cluster.
- Run `kubectl get pvc` to see our pvcs.
  - `meep-dataset-v2` contains our raw data. This was generated by a meep script in the repo [general_3x3](https://github.com/Kovaleski-Research-Lab/general_3x3/tree/andy_branch).
  - `dft-volumes` contains reduced data. The script used for this task is at [general 3x3 data reduction](https://github.com/Kovaleski-Research-Lab/general_3x3/blob/andy_branch/preprocess_data.py).
  - `preprocessed-data` contains data that has been prepped for the time-series networks. The script used for this task is at meta_atom_rnn/preprocess_data/preprocess.py.
  - `training-results` contains model checkpoints from training the models, organized by experiment. It also contains analysis, including loss plots, truth/pred images, etc.

- We can enter into a pod that is mounted to our PVCs using interactive mode.
  - .yaml files for interactive pods are located at `meta_atom_rnn/k8s/monitor_pods`
    - Create the pod using `kubectl apply -f {filename}.yaml`
    - Enter the pod using `kubectl exec -it {pod_name} -- /bin/bash`
        - Note: interactive pods only last 6 hours. If a pod expires, it needs to be deleted: `kubectl delete pod {pod_name}` and then recreated.

### Launching jobs

- Run a container from our [launcher image](https://hub.docker.com/layers/kovaleskilab/meep_ml/launcher/images/sha256-464ec5f4310603229e96b5beae9355055e2fb2de2027539c3d6bef94b7b5a4f1?context=repo).
- `cd /develop/code/meta_atom_rnn/k8s`
  
#### Launch data preprocessing

- in configs/params.yaml, set `deployment_mode` to 1 and `experiment` to 0
- `python3 launch_preprocess.py -config configs/params.yaml` 

#### Launch training

- in configs/params.yaml, set `deployment_mode` to 1 and `experiment` to 1
- `python3 launch_training.py -config configs/params.yaml`

#### Load results

- in configs/params.yaml, set `deployment_mode` to 1 and `experiment` to 2
- `python3 launch_evaluation -config configs/params.yaml`

#### Run evaluation

- in configs/params.yaml, set `deployment_mode` to 1 and `experiment` to 3
- `python3 launch_evaluation -config configs/params.yaml`

## Monitor kubernetes tasks
- `kubectl get job` : shows all jobs
  - Each job will spawn a pod (i.e., container) to run a script / complete a task.
- `kubectl get pod` : shows all pods
- `kubectl describe pod [pod_name]` : shows details of the creation process for a pod. Pods take time to launch.
    - `[pod_name]` is the name of the pod shown in `kubectl get pod` above
- `kubectl logs [pod_name]` gives you stdout printouts.
  

